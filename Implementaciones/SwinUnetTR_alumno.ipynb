{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion SwinUnetTR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figs/swin_unetr.png\" width=\"60%\"/>\n",
    "\n",
    "Algunos detalles:\n",
    "\n",
    "* Emplea LeakyReLU en lugar de ReLU\n",
    "* Normaliza con instance normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Union, Sequence, Tuple\n",
    "from monai.utils import ensure_tuple_rep\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaci√≥n resblock\n",
    "\n",
    "<img  src=\"figs/res_block.png\" width=\"50%\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloque residual como el de la figura anterior. La normalizacion que emplea es InstanceNorm\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions.\n",
    "        in_channels: number of input channels.\n",
    "        out_channels: number of output channels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        kernel_size =  3\n",
    "\n",
    "        self.adjust_in_channels = in_channels != out_channels\n",
    "\n",
    "        if spatial_dims == 2:\n",
    "            # Completar\n",
    "            if self.adjust_in_channels:\n",
    "                # Completar\n",
    "        else:\n",
    "            #repetir para 3D\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        residual = inp\n",
    "        \n",
    "\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobacion resblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets.swin_unetr import SwinUNETR\n",
    "\n",
    "model = SwinUNETR(128,4,3,feature_size = 48)\n",
    "\n",
    "print(model.encoder2)\n",
    "print(\"num of parameters: \", sum(p.numel() for p in model.encoder2.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resblock = ResBlock(3, 48, 48)\n",
    "print(resblock)\n",
    "print(\"num of parameters: \", sum(p.numel() for p in resblock.parameters() if p.requires_grad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder layer for swin unetr. \n",
    "        1- Upsamples the x_down image\n",
    "        2- Cats the upsampled image with x in de channel dimmension\n",
    "        3- Applies a resblock to the cat image (number of channels of the residual block is 2*in_channels )\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions.\n",
    "        in_channels_down: number of  channels lower level.\n",
    "        in_channels: number of  channels upperlevel.\n",
    "        out_channels: number of output channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        in_channels_down: int,\n",
    "        out_channels: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        upsample_kernel_size =  2\n",
    "        stride = 2\n",
    "\n",
    "\n",
    "\n",
    "        if spatial_dims == 2:\n",
    "            \n",
    "        else:\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    " \n",
    "    def forward(self, x_down, x):\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.decoder5)\n",
    "print(\"num of parameters: \", sum(p.numel() for p in model.decoder5.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder5 = Decoder(3, 384, 768, 384)\n",
    "print(decoder5)\n",
    "print(\"num of parameters: \", sum(p.numel() for p in decoder5.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mySwinUNETR(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin UNETR based on: \"Hatamizadeh et al.,\n",
    "    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n",
    "    <https://arxiv.org/abs/2201.01266>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: Union[Sequence[int], int],\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        depths: Sequence[int] = (2, 2, 2, 2),\n",
    "        num_heads: Sequence[int] = (3, 6, 12, 24),\n",
    "        feature_size: int = 24,\n",
    "        norm_name: Union[Tuple, str] = \"instance\",\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        dropout_path_rate: float = 0.0,\n",
    "        normalize: bool = True,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size: dimension of input image.\n",
    "            in_channels: dimension of input channels.\n",
    "            out_channels: dimension of output channels.\n",
    "            feature_size: dimension of network feature size.\n",
    "            depths: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            norm_name: feature normalization type and arguments.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            dropout_path_rate: drop path rate.\n",
    "            normalize: normalize output intermediate features in each stage.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: number of spatial dims.\n",
    "            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n",
    "                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n",
    "                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # for 3D single channel input with size (96,96,96), 4-channel output and feature size of 48.\n",
    "            >>> net = SwinUNETR(img_size=(96,96,96), in_channels=1, out_channels=4, feature_size=48)\n",
    "\n",
    "            # for 3D 4-channel input with size (128,128,128), 3-channel output and (2,4,2,2) layers in each stage.\n",
    "            >>> net = SwinUNETR(img_size=(128,128,128), in_channels=4, out_channels=3, depths=(2,4,2,2))\n",
    "\n",
    "            # for 2D single channel input with size (96,96), 2-channel output and gradient checkpointing.\n",
    "            >>> net = SwinUNETR(img_size=(96,96), in_channels=3, out_channels=2, use_checkpoint=True, spatial_dims=2)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = ensure_tuple_rep(img_size, spatial_dims)\n",
    "        patch_size = ensure_tuple_rep(2, spatial_dims)\n",
    "        window_size = ensure_tuple_rep(7, spatial_dims)\n",
    "\n",
    "        if not (spatial_dims == 2 or spatial_dims == 3):\n",
    "            raise ValueError(\"spatial dimension should be 2 or 3.\")\n",
    "\n",
    "        for m, p in zip(img_size, patch_size):\n",
    "            for i in range(5):\n",
    "                if m % np.power(p, i + 1) != 0:\n",
    "                    raise ValueError(\"input image size (img_size) should be divisible by stage-wise image resolution.\")\n",
    "\n",
    "        if not (0 <= drop_rate <= 1):\n",
    "            raise ValueError(\"dropout rate should be between 0 and 1.\")\n",
    "\n",
    "        if not (0 <= attn_drop_rate <= 1):\n",
    "            raise ValueError(\"attention dropout rate should be between 0 and 1.\")\n",
    "\n",
    "        if not (0 <= dropout_path_rate <= 1):\n",
    "            raise ValueError(\"drop path rate should be between 0 and 1.\")\n",
    "\n",
    "        if feature_size % 12 != 0:\n",
    "            raise ValueError(\"feature_size should be divisible by 12.\")\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.swinViT = SwinTransformer(\n",
    "            in_chans=in_channels,\n",
    "            embed_dim=feature_size,\n",
    "            window_size=window_size,\n",
    "            patch_size=patch_size,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=4.0,\n",
    "            qkv_bias=True,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            drop_path_rate=dropout_path_rate,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            spatial_dims=spatial_dims,\n",
    "            downsample=downsample,\n",
    "        )\n",
    "\n",
    "        # rama de arriba aumenta dimensiones de 4 a 48\n",
    "        self.encoder1 = \n",
    "\n",
    "        # lo aplica al resultado del patch encoding resolucion 1/2\n",
    "        self.encoder2 = \n",
    "\n",
    "        # lo aplica al resultado del patch encoding resolucion 1/4\n",
    "        self.encoder3 = \n",
    "\n",
    "        # lo aplica al resultado del patch encoding resolucion 1/8\n",
    "        self.encoder4 = \n",
    "\n",
    "        # NO HAY ENCODER 5!!! LA FIGURA ESTA MAL\n",
    "\n",
    "        # lo aplica al resultado del patch encoding resolucion 1/32\n",
    "        self.bottleneck = \n",
    "\n",
    "        # Combina salida de bottleneck con la salida del nivel 3 del SwinViT\n",
    "        self.decoder5 = \n",
    "\n",
    "\n",
    "        self.decoder4 = \n",
    "\n",
    "        self.decoder3 = \n",
    "\n",
    "        self.decoder2 = \n",
    "\n",
    "        self.decoder1 = \n",
    "\n",
    "\n",
    "        if spatial_dims == 2:\n",
    "            self.out = nn.Conv2d( # type: ignore\n",
    "                in_channels=feature_size ,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                bias=True,\n",
    "            )\n",
    "        else:\n",
    "            self.out = nn.Conv3d( # type: ignore\n",
    "                in_channels=feature_size ,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                bias=True,\n",
    "            )\n",
    "   \n",
    "    def forward(self, x_in):\n",
    "        \n",
    "        return logits\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = mySwinUNETR(128,4,3,feature_size = 48)\n",
    "\n",
    "print(\"num of parameters: \", sum(p.numel() for p in my_model.parameters() if p.requires_grad))\n",
    "\n",
    "modules = [my_model.swinViT, my_model.encoder1, my_model.encoder2, my_model.encoder3, my_model.encoder4, my_model.bottleneck, my_model.decoder5, my_model.decoder4, my_model.decoder3, my_model.decoder2, my_model.decoder1, my_model.out]\n",
    "\n",
    "for module in modules:\n",
    "    print(\"num of parameters: \", sum(p.numel() for p in module.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinUNETR(128,4,3,feature_size = 48)\n",
    "\n",
    "\n",
    "print(\"num of parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "modules = [model.swinViT, model.encoder1, model.encoder2, model.encoder3, model.encoder4, model.encoder10, model.decoder5, model.decoder4, model.decoder3, model.decoder2, model.decoder1, model.out]\n",
    "for module in modules:\n",
    "    print(\"num of parameters: \", sum(p.numel() for p in module.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_model.decoder2)\n",
    "print(model.decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_model.encoder1)\n",
    "print(model.encoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m1, m2 in zip(model.swinViT.named_parameters(), my_model.swinViT.named_parameters()):\n",
    "    if m1[1].shape != m2[1].shape:\n",
    "        print(m1[0], m1[1].shape)\n",
    "        print(m2[0], m2[1].shape)\n",
    "\n",
    "print(\"num of parameters: \", sum(p.numel() for p in model.swinViT.parameters() if p.requires_grad))\n",
    "print(\"num of parameters: \", sum(p.numel() for p in my_model.swinViT.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.swinViT.named_parameters():\n",
    "    \n",
    "    print(m[0], m[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CursoTransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:35:26) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "332768db682295b8dac3c4a901a1fe1343c13865b4033bb10f86fd07133ed44e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
