{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportGeneralTypeIssues=false\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the MedFormer model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver todos los detalles podéis descargar el  [paper](https://arxiv.org/pdf/2203.00131.pdf) \n",
    "\n",
    "El diagrama de arquitectura del modelo es el siguiente:\n",
    "\n",
    "![Arquitectura del modelo](figs/medformer.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional stem \n",
    "\n",
    "El convolutional stem consiste en una convolución seguida de un bloque residual como se muestra en la siguiente figura:\n",
    "\n",
    "![Convolutional stem](figs/convolutional_stem.png)\n",
    "\n",
    "Para el bloque residual hay varias opciones, en este caso se ha elegido el siguiente:\n",
    "\n",
    "![Residual block](figs/residual_block1.png)\n",
    "\n",
    "Comenzaremos implementando el bloque residual que luego tambien emplearemos en varios puntos, y antes de este la secuencia basica norm->activation->conv:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyright: reportGeneralTypeIssues=false\n",
    "from typing import Union, List\n",
    "\n",
    "class NormActConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalization, activation and convolution layer \n",
    "    \n",
    "    Args:\n",
    "        spatial_dims (int): number of spatial dimensions\n",
    "        in_ch (int): number of input channels\n",
    "        out_ch (int): number of output channels\n",
    "        kernel_size (int): kernel size for convolution\n",
    "        stride (int): stride for convolution\n",
    "        padding (int): padding for convolution\n",
    "        groups (int): groups for convolution\n",
    "        dilation (int): dilation for convolution\n",
    "        bias (bool): bias for convolution\n",
    "        norm (nn.Module): normalization layer (nn.BatchNorm3d, nn.InstanceNorm3d, nn.Identity) also 2d equivelents\n",
    "        act (nn.Module): activation layer (nn.ReLU,nn.GELU,nn.Identity)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spatial_dims,  in_ch, out_ch, padding = 1, kernel_size=3, stride = 1 ,\n",
    "        groups=1, dilation=1, bias=False, norm=nn.InstanceNorm3d, act =nn.GELU):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = norm(in_ch) if norm!=nn.Identity else nn.Identity()\n",
    "        self.act = act() \n",
    "\n",
    "        if spatial_dims == 2:\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels=in_ch, \n",
    "                out_channels=out_ch, \n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "                dilation=dilation,\n",
    "                bias=bias\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Conv3d(\n",
    "                in_channels=in_ch, \n",
    "                out_channels=out_ch, \n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "                dilation=dilation,\n",
    "                bias=bias\n",
    "            )\n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "    \n",
    "        return  self.conv(self.act(self.norm(x)))\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación del bloque residual:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#right: reportGeneralTypeIssues=false\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,spatial_dims,  in_ch, out_ch, kernel_size=[3,3,3], stride=1, norm=nn.InstanceNorm3d, act=nn.GELU, preact=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        pad_size = [i//2 for i in kernel_size]\n",
    "\n",
    "        self.conv1 = NormActConv(spatial_dims, in_ch, out_ch, padding=pad_size, kernel_size = kernel_size, stride=stride,  norm=norm, act=act)\n",
    "        self.conv2 = NormActConv(spatial_dims, out_ch, out_ch,  padding=pad_size, kernel_size = kernel_size, stride=1, norm=norm, act=act)\n",
    "\n",
    "        \n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            # ESTO ES LA VERSION ORIGINAL NO ES MUY ORTODOXO POR LA ACTIVACION\n",
    "            #self.shortcut = NormActConv(in_ch, out_ch, kernel_size, stride=stride, padding=pad_size, norm=norm, act=act)\n",
    "            \n",
    "            # Hay que poner activacion identity\n",
    "            self.shortcut = NormActConv(spatial_dims, in_ch, out_ch, kernel_size=1, stride=stride, padding=0, bias=False, act = nn.Identity)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya tenemos los bloques necesarios para el convolutional stem, que consiste en una convolución seguida de un bloque residual, como en la figura de arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvStem(nn.Module):\n",
    "    \"\"\" Convolutional stem: \n",
    "            sequential of conv and redidual block\"\"\"\n",
    "    def __init__(self, spatial_dims,  in_ch, out_ch, kernel_size=3,  norm=nn.InstanceNorm3d, act=nn.GELU):\n",
    "        super().__init__()\n",
    "\n",
    "        pad_size = kernel_size // 2 \n",
    "        #inicializar modulos\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #completar\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down block\n",
    "\n",
    "El down block es el encargado de reducir la dimensión en el encoder y generar los semantic maps:\n",
    "\n",
    "<img src=\"figs/down_block.png\" width=\"40%\"/>\n",
    "\n",
    "El primer bloque, patch_merging fusiona parches de la siguiente forma:\n",
    "\n",
    "<img src=\"figs/patch_merging.png\" width=\"40%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Sequential):\n",
    "    def __init__(self, spatial_dims, in_ch, out_ch, stride=1, kernel_size=3, bias=False):\n",
    "        \n",
    "        \n",
    "        if isinstance(kernel_size, list):\n",
    "            padding = [i//2 for i in kernel_size]\n",
    "        else:\n",
    "            padding = kernel_size // 2\n",
    "        if spatial_dims == 2:\n",
    "            super().__init__(nn.Conv2d(\n",
    "                in_channels=in_ch,\n",
    "                out_channels=in_ch,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=in_ch,\n",
    "                bias=bias),\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_ch,\n",
    "                out_channels=out_ch,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                groups=1,\n",
    "                bias=bias)\n",
    "            )\n",
    "        else:\n",
    "            super().__init__(nn.Conv3d(\n",
    "                in_channels=in_ch,\n",
    "                out_channels=in_ch,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                groups=in_ch,\n",
    "                bias=bias),\n",
    "            nn.Conv3d(\n",
    "                in_channels=in_ch,\n",
    "                out_channels=out_ch,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                groups=1,\n",
    "                bias=bias)\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified patch merging layer that works as down-sampling\n",
    "\n",
    "    Args:\n",
    "        dim (int): number of input channels\n",
    "        out_dim (int): number of output channels\n",
    "        norm (nn.Module): normalization layer (nn.BatchNorm3d, nn.InstanceNorm3d, nn.Identity) also 2d equivelents\n",
    "        proj_type (str): projection type ('linear' or 'depthwise')\n",
    "        down_scale (list): down-sampling scale for each dimension\n",
    "        kernel_size (list): kernel size for \n",
    "        wise separable convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, out_dim, norm=nn.InstanceNorm3d, proj_type='linear', down_scale=[2,2,2], kernel_size=[3,3,3]):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        assert proj_type in ['linear', 'depthwise']\n",
    "\n",
    "        spatial_dims = len(down_scale)\n",
    "        self.down_scale = down_scale\n",
    "\n",
    "        merged_dim = int(np.prod(down_scale) * dim)\n",
    "\n",
    "        if proj_type == 'linear':\n",
    "            self.reduction = nn.Conv3d(merged_dim, out_dim, kernel_size=1, bias=False)\n",
    "        else:\n",
    "            self.reduction = DepthwiseSeparableConv(spatial_dims, merged_dim, out_dim, kernel_size=kernel_size)\n",
    "\n",
    "        self.norm = norm(merged_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, C, D, H, W\n",
    "        \"\"\"\n",
    "        #merged_x = []\n",
    "        #for i in range(self.down_scale[0]):\n",
    "        #    for j in range(self.down_scale[1]):\n",
    "        #        for k in range(self.down_scale[2]):\n",
    "        #            tmp_x = x[:, :, i::self.down_scale[0], j::self.down_scale[1], k::self.down_scale[2]]\n",
    "        #            merged_x.append(tmp_x)\n",
    "        \n",
    "        #x = torch.cat(merged_x, 1)\n",
    "\n",
    "        #reimplementar con rearrange\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  4,  5, 16, 17, 20, 21])\n",
      "tensor([ 0,  1,  4,  5, 16, 17, 20, 21])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 4*4*4).reshape(1, 1, 4, 4, 4)\n",
    "\n",
    "#completar para comprobar\n",
    "x1 = rearrange(x, '', d1=2, d2=2, h1=2, h2=2, w1=2, w2=2)\n",
    "print(x1[0,:,0,0,0])\n",
    "\n",
    "merged_x = []\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            tmp_x = x[:, :, i::2, j::2, k::2]\n",
    "            merged_x.append(tmp_x)\n",
    "        \n",
    "x2 = torch.cat(merged_x, 1)\n",
    "\n",
    "print(x2[0,:,0,0,0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Transformer Block\n",
    "\n",
    "Este bloque tiene Multihead Attention comenzaremos por implementar con single head attention y luego lo extenderemos a multihead attention.\n",
    "\n",
    "![Bidirectional Transformer Block](figs/bidirectional_transformer.png)\n",
    "\n",
    "\n",
    "Notas para la implementacion:\n",
    "\n",
    "* En lugar de hacer dos convoluciones (para Q/K y V) hacemos una con doble numero de canales y separamos (torch.split, torch.chunk)\n",
    "\n",
    "* Añade dropout en dos lugares: a la matriz de atencion de M_out y a la salida de X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class BiDirectionalAtt2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Head BiDirectional Attention block in 2D.\n",
    "\n",
    "    Args:\n",
    "        feat_dim: Number of input X feature channels.\n",
    "        map_dim: number of M feature channels. (equal input and output)\n",
    "        out_channels: number of output X feature channels.\n",
    "        kernel_size: convolution kernel size.\n",
    "        attn_drop: dropout rate for M attention map.\n",
    "        proj_drop: dropout rate for X output.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feat_dim, map_dim, head_dim, out_dim,  attn_drop=0., proj_drop=0.,\n",
    "                     kernel_size=[3,3]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.feat_dim = feat_dim\n",
    "        self.map_dim = map_dim\n",
    "        self.scale = feat_dim ** (-0.5)\n",
    "        self.dim_head = feat_dim\n",
    "        spatial_dims = 2\n",
    "\n",
    "        self.X_qv = DepthwiseSeparableConv(spatial_dims,feat_dim, self.head_dim*2, kernel_size=kernel_size)\n",
    "        self.X_out = DepthwiseSeparableConv(spatial_dims, self.head_dim, out_dim, kernel_size=kernel_size)\n",
    "\n",
    "        self.M_qv = nn.Conv2d(map_dim, self.head_dim*2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.M_out = nn.Conv2d(self.head_dim, map_dim, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "\n",
    "    def forward(self, X, M):\n",
    "\n",
    "        B, C, H, W = X.shape\n",
    "        b, c, h, w = M.shape\n",
    "        \n",
    "        X_qv = self.X_qv(X)\n",
    "        M_qv = self.M_qv(M)\n",
    "        \n",
    "\n",
    "        X_qk, X_v = torch.chunk(X_qv, 2, dim = 1)  # B, inner_dim, H, W\n",
    "        M_qk, M_v = torch.split(M_qv,self.head_dim, dim = 1)  # B, inner_dim, H, W\n",
    "\n",
    "        \n",
    "        # completar\n",
    "\n",
    "        return X_out, M_out\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobacion de la implementacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256, 15]) torch.Size([5, 16, 15])\n",
      "torch.Size([5, 30, 16, 16]) torch.Size([5, 10, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(5,20, 16,16)\n",
    "M = torch.rand(5,10,4,4)\n",
    "\n",
    "biAttn = BiDirectionalAtt2D(20,10,15, 30)\n",
    "\n",
    "X_out, M_out = biAttn(X,M)\n",
    "\n",
    "print(X_out.shape, M_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional attention en 2D y 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class BiDirectionalAtt(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Head BiDirectional Attention block in 2D and 3D.\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: Number of spatial dimensions.\n",
    "        feat_dim: Number of input X feature channels.\n",
    "        map_dim: number of M feature channels. (equal input and output)\n",
    "        head_dim: dimension of head features\n",
    "        out_dim: number of output X feature channels.\n",
    "        kernel_size: convolution kernel size.\n",
    "        attn_drop: dropout rate for M attention map.\n",
    "        proj_drop: dropout rate for X output.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,spatial_dims,  feat_dim, map_dim, head_dim, out_dim,  attn_drop=0., proj_drop=0.,\n",
    "                     kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.head_dim = head_dim\n",
    "        self.feat_dim = feat_dim\n",
    "        self.map_dim = map_dim\n",
    "        self.scale = head_dim ** (-0.5)\n",
    "        self.dim_head = head_dim\n",
    "        \n",
    "        self.X_qv = DepthwiseSeparableConv(spatial_dims,feat_dim, self.head_dim*2, kernel_size=kernel_size)\n",
    "        self.X_out = DepthwiseSeparableConv(spatial_dims, self.head_dim, out_dim, kernel_size=kernel_size)\n",
    "\n",
    "        if self.spatial_dims ==2:\n",
    "            self.M_qv = nn.Conv2d(map_dim, self.head_dim*2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            self.M_out = nn.Conv2d(self.head_dim, map_dim, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        else:\n",
    "            self.M_qv = nn.Conv3d(map_dim, self.head_dim*2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            self.M_out = nn.Conv3d(self.head_dim, map_dim, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "\n",
    "    def forward(self, X, M):\n",
    "\n",
    "        if self.spatial_dims ==2:\n",
    "            B, C, H, W = X.shape\n",
    "            b, c, h, w = M.shape\n",
    "            d = D = None\n",
    "        else:\n",
    "            B, C, D, H, W = X.shape\n",
    "            b, c, d, h, w = M.shape\n",
    "\n",
    "        X_qv = self.X_qv(X)\n",
    "        M_qv = self.M_qv(M)\n",
    "        \n",
    "\n",
    "        X_qk, X_v = torch.chunk(X_qv, 2, dim = 1)  # B, inner_dim, H, W\n",
    "        M_qk, M_v = torch.split(M_qv,self.head_dim, dim = 1)  # B, inner_dim, H, W\n",
    "\n",
    "       #Copiar del anterior y modificar\n",
    "\n",
    "        return X_out, M_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 25, 16, 16]) torch.Size([5, 10, 4, 4])\n",
      "torch.Size([5, 25, 16, 16, 16]) torch.Size([5, 10, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(5,20, 16,16)\n",
    "M = torch.rand(5,10,4,4)\n",
    "\n",
    "\n",
    "spatial_dims = 2\n",
    "feat_dim = 20\n",
    "map_dim = 10\n",
    "head_dim = 15\n",
    "out_dim = 25\n",
    "\n",
    "biAttn2D = BiDirectionalAtt(spatial_dims,  feat_dim, map_dim, head_dim, out_dim)\n",
    "\n",
    "X_out, M_out = biAttn2D(X,M)\n",
    "\n",
    "print(X_out.shape, M_out.shape)\n",
    "\n",
    "\n",
    "X = torch.rand(5,20, 16, 16,16)\n",
    "M = torch.rand(5,10,4, 4,4)\n",
    "\n",
    "spatial_dims = 3\n",
    "feat_dim = 20\n",
    "map_dim = 10\n",
    "head_dim = 15\n",
    "out_dim = 25\n",
    "biAttn3D = BiDirectionalAtt(spatial_dims,  feat_dim, map_dim, head_dim, out_dim)\n",
    "\n",
    "X_out, M_out = biAttn3D(X,M)\n",
    "\n",
    "print(X_out.shape, M_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multihead bidirectional attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class MultiHeadBiDirectionalAtt(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Head BiDirectional Attention block in 2D and 3D.\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: Number of spatial dimensions.\n",
    "        feat_dim: Number of input X feature channels.\n",
    "        num_heads: Number of attention heads.\n",
    "        head_dim: Number of attention head channels.\n",
    "        map_dim: number of M feature channels. (equal input and output)\n",
    "        out_dim: number of output X feature channels.\n",
    "        attn_drop: dropout rate for M attention map.\n",
    "        proj_drop: dropout rate for X output.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,spatial_dims,  feat_dim, map_dim, num_heads, head_dim,  out_dim,  attn_drop=0., proj_drop=0.,\n",
    "                     kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.feat_dim = feat_dim\n",
    "        self.map_dim = map_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.inner_dim = head_dim * num_heads\n",
    "        self.scale = feat_dim ** (-0.5)\n",
    "        \n",
    "        \n",
    "        self.X_qv = DepthwiseSeparableConv(spatial_dims,feat_dim, self.inner_dim*2, kernel_size=kernel_size)\n",
    "        self.X_out = DepthwiseSeparableConv(spatial_dims, self.inner_dim, out_dim, kernel_size=kernel_size)\n",
    "\n",
    "        if self.spatial_dims ==2:\n",
    "            self.M_qv = nn.Conv2d(map_dim, self.inner_dim*2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            self.M_out = nn.Conv2d(self.inner_dim, map_dim, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        else:\n",
    "            self.M_qv = nn.Conv3d(map_dim, self.inner_dim*2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            self.M_out = nn.Conv3d(self.inner_dim, map_dim, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "\n",
    "    def forward(self, X, M):\n",
    "\n",
    "        if self.spatial_dims ==2:\n",
    "            B, C, H, W = X.shape\n",
    "            b, c, h, w = M.shape\n",
    "            D = d = None\n",
    "        elif self.spatial_dims ==3:\n",
    "            B, C, D, H, W = X.shape\n",
    "            b, c, d, h, w = M.shape\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        X_qv = self.X_qv(X)\n",
    "        M_qv = self.M_qv(M)\n",
    "        \n",
    "\n",
    "        X_qk, X_v = torch.chunk(X_qv, 2, dim = 1)  # B, inner_dim, (D) H, W\n",
    "        M_qk, M_v = torch.split(M_qv,self.inner_dim, dim = 1)  # B, inner_dim, (D) H, W\n",
    "\n",
    "        #copiar del anterior y modificar para incluir las heads\n",
    "\n",
    "        return X_out, M_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 25, 16, 16]) torch.Size([5, 10, 4, 4])\n",
      "torch.Size([5, 25, 16, 16, 16]) torch.Size([5, 10, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(5,20, 16,16)\n",
    "M = torch.rand(5,10,4,4)\n",
    "\n",
    "\n",
    "spatial_dims = 2\n",
    "feat_dim = 20\n",
    "map_dim = 10\n",
    "head_dim = 15\n",
    "out_dim = 25\n",
    "num_heads = 4\n",
    "\n",
    "\n",
    "biAttn2D = MultiHeadBiDirectionalAtt(spatial_dims,  feat_dim, map_dim, num_heads, head_dim, out_dim)\n",
    "\n",
    "X_out, M_out = biAttn2D(X,M)\n",
    "\n",
    "print(X_out.shape, M_out.shape)\n",
    "\n",
    "\n",
    "X = torch.rand(5,20, 16, 16,16)\n",
    "M = torch.rand(5,10,4, 4,4)\n",
    "\n",
    "spatial_dims = 3\n",
    "feat_dim = 20\n",
    "map_dim = 10\n",
    "head_dim = 15\n",
    "out_dim = 25\n",
    "num_heads = 5\n",
    "\n",
    "biAttn3D = MultiHeadBiDirectionalAtt(spatial_dims,  feat_dim, map_dim, num_heads, head_dim, out_dim)\n",
    "\n",
    "X_out, M_out = biAttn3D(X,M)\n",
    "\n",
    "print(X_out.shape, M_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Transformer Block\n",
    "\n",
    "Una vez tenemos la atención bidireccional el transformer bidireccional añade las partes de atencion y Feed Forward como en un transformer normal.\n",
    "\n",
    "<img src=\"figs/bidirectional_transformer_layer1.png\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionAttentionBlock(nn.Module):\n",
    "    \"\"\"Bidirectional Attention Block\n",
    "    Args:\n",
    "        feat_dim (int): number of channels of feature map\n",
    "        map_dim (int): number of channels of semantic map\n",
    "        out_dim (int): number of channels of output feature map\n",
    "        heads (int): number of heads\n",
    "        dim_head (int): dimension of each head\n",
    "        norm (nn.Module, optional): normalization layer. Defaults to nn.InstanceNorm3d.\n",
    "        act (nn.Module, optional): activation layer. Defaults to nn.GELU.\n",
    "        expansion (int, optional): expansion ratio of MLP. Defaults to 4.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim, map_dim, out_dim, heads, dim_head, norm=nn.InstanceNorm3d, act=nn.GELU,\n",
    "                expansion=4, attn_drop=0., proj_drop=0., map_size=[8, 8, 8], \n",
    "                proj_type='depthwise', kernel_size=[3,3,3]):\n",
    "        super().__init__()\n",
    "\n",
    "        spatial_dims = len(map_size)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(feat_dim)  # norm layer for feature map\n",
    "        self.norm2 = nn.LayerNorm(map_dim) if norm else nn.Identity() # norm layer for semantic map\n",
    "        \n",
    "        self.attn = MultiHeadBiDirectionalAtt(spatial_dims, feat_dim, map_dim, out_dim, heads, dim_head, attn_drop=attn_drop, proj_drop=proj_drop, map_size=map_size, proj_type=proj_type, kernel_size=kernel_size)\n",
    "\n",
    "        \n",
    "        if feat_dim != out_dim:\n",
    "            self.shortcut = NormActConv(spatial_dims, feat_dim, out_dim, 1, padding=0, norm=norm, act=nn.Identity)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        if proj_type == 'linear':\n",
    "            self.feedforward = FusedMBConv(out_dim, out_dim, expansion=expansion, kernel_size=1, act=act, norm=norm)\n",
    "        else:\n",
    "            self.feedforward = MBConv(out_dim, out_dim, expansion=expansion, kernel_size=kernel_size, act=act, norm=norm)\n",
    "\n",
    "    def forward(self, x, semantic_map):\n",
    "        \n",
    "        feat = self.norm1(x)\n",
    "        mapp = self.norm2(semantic_map)\n",
    "\n",
    "        out, mapp = self.attn(feat, mapp)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = self.feedforward(out)\n",
    "\n",
    "        mapp += semantic_map\n",
    "\n",
    "        return out, mapp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadBiDirectionalAttLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi head bi-directional attention layer (repeats MultiHeadBiDirectionalAtt for num_blocks times)\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: Number of spatial dimensions.\n",
    "        feat_dim: Number of input X feature channels.\n",
    "        num_heads: Number of attention heads.\n",
    "        head_dim: Number of attention head channels.\n",
    "        map_dim: number of M feature channels. (equal input and output)\n",
    "        out_dim: number of output X feature channels.\n",
    "        attn_drop: dropout rate for M attention map.\n",
    "        proj_drop: dropout rate for X output.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, spatial_dims, feat_dim, map_dim, out_dim, num_blocks, heads=4, dim_head=64, expansion=4, attn_drop=0., proj_drop=0., map_size=[8,8,8], proj_type='depthwise', act=nn.GELU, kernel_size=[3,3,3]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_dims = spatial_dims\n",
    "        dim1 = feat_dim\n",
    "        dim2 = out_dim\n",
    "\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(MultiHeadBiDirectionalAtt(spatial_dims, \n",
    "                        feat_dim, map_dim, out_dim, heads, dim_head, attn_drop=attn_drop, proj_drop=proj_drop, kernel_size=kernel_size))\n",
    "            dim1 = out_dim\n",
    "\n",
    "    def forward(self, x, semantic_map):\n",
    "        for block in self.blocks:\n",
    "            x, semantic_map = block(x, semantic_map)\n",
    "\n",
    "        \n",
    "        return x, semantic_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic map generation\n",
    "\n",
    "Seguiremos con el bloque de la Figura 8.\n",
    "\n",
    "<img src=\"figs/semantic_map_generation.png\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticMapGeneration(nn.Module):\n",
    "    \"\"\"\n",
    "    Semantic Map Generation Module\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: Number of spatial dimensions.\n",
    "        feat_dim: Number of input X feature channels.\n",
    "        w: Semantic map width.\n",
    "        h: Semantic map height.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,spatial_dims,  feat_dim, w, h, d = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.feat_dim = feat_dim\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.d = d\n",
    "\n",
    "\n",
    "        if self.spatial_dims ==2:\n",
    "            self.weight_map_size = w * h \n",
    "            self.conv = nn.Conv2d(self.feat_dim, self.feat_dim + self.w * self.h , kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        else:\n",
    "            self.weight_map_size = w * h * d\n",
    "            self.conv = nn.Conv3d(self.feat_dim, self.feat_dim +  self.w * self.h * self.d, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        tmp  = self.conv(X)\n",
    "        \n",
    "        token_map, weight_map  = torch.split(tmp, [self.feat_dim, self.weight_map_size], dim=1)\n",
    "\n",
    "        if self.spatial_dims ==2:\n",
    "            weight_map = rearrange(weight_map, 'b c h w -> b c (h w)')\n",
    "            token_map = rearrange(token_map, 'b c h w -> b c (h w)')\n",
    "        else:\n",
    "            weight_map = rearrange(weight_map, 'b c d w h -> b c (d w h)')\n",
    "            token_map = rearrange(token_map, 'b c d w h  -> b c (d w h)')\n",
    "        \n",
    "        weight_map = torch.softmax(weight_map, dim=-1)\n",
    "\n",
    "        print(weight_map.shape, token_map.shape)\n",
    "        out = torch.einsum('bsS, bcS->bcs', weight_map, token_map)\n",
    "        if self.spatial_dims ==2:\n",
    "            out = rearrange(out, 'b c (h w) -> b c h w', h=self.h, w=self.w)\n",
    "        else:\n",
    "            out = rearrange(out, 'b c (d h w) -> b c d h w', h=self.h, w=self.w, d=self.d)\n",
    "        return out\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64, 4096]) torch.Size([5, 20, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64, 20])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 64, 4096)\n",
    "y = torch.rand(5, 20, 4096)\n",
    "print(x.shape, y.shape)\n",
    "torch.einsum('b s S,b c S->b c s', y, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16, 256]) torch.Size([5, 20, 256])\n",
      "torch.Size([5, 20, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "spatial_dims = 2\n",
    "feat_dim = 20\n",
    "w = 4\n",
    "h = 4\n",
    "d = 4\n",
    "\n",
    "semanticMap = SemanticMapGeneration(spatial_dims,  feat_dim, w, h, d)\n",
    "\n",
    "X = torch.rand(5,20, 16, 16)\n",
    "\n",
    "y = semanticMap(X)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic map fusion\n",
    "\n",
    "Seguiremos con el bloque de la Figura 8.C\n",
    "\n",
    "\n",
    "<img src=\"figs/semantic_map_fusion.png\" width=\"30%\"/>\n",
    "\n",
    "La explicacion de este bloque es la siguiente:\n",
    "Multi-scale fusion plays a vital role in dense prediction tasks\n",
    "to combine the high-level semantic and low-level detailed information. The semantic map in BMHA is naturally suitable for multi-scale fusion with a minimal computation overhead. See in Fig.\n",
    "1 (C), given 2D semantic maps from multiple scales: M1, M2, . . . , Mn, we first flatten them and\n",
    "concatenate them together into a long 1D token sequence MF . The sequence MF contains all tokens from all scales and is then fed into conventional Transformer blocks for multi-scale semantic\n",
    "fusion. The fused sequence is then chunked and reshaped back to 2D semantic maps. Unlike previous approaches fuse multi-scale features locally, such as fusing with resized multi-scale feature9 or\n",
    "with atrous spatial pyramid pooling47, the proposed approach propagates information across all tokens at every scale via the all-to-all attention to form a semantically and spatially global multi-scale\n",
    "fusion.\n",
    "\n",
    "Algun comentario:\n",
    "\n",
    "* Teoricamente cada semantic map de cada resolucion tiene una dimension diferente\n",
    "* Antes de concatenar los tokens de cada resolucion se proyectan a una dimension comun \"dim\"\n",
    "* Tras el transformer cada bloque se lleva a la dimension original de cada resolucion\n",
    "* en la implementacion original distingue entre dropout de attencion y de FF, aunque luego solo usa el de FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticMapFusion(nn.Module):\n",
    "    \"\"\"semantic map fusion module\n",
    "    Args:\n",
    "        spatial_dims: Number of spatial dimensions.\n",
    "        in_dim_list: number of input feature channels for each resolution\n",
    "        dim: Number of feature channels \n",
    "        heads: Number of attention heads.\n",
    "        depth: Number of transformer blocks.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spatial_dims, in_dim_list, dim, heads, depth=1,  dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.dim = dim\n",
    "\n",
    "        # project all maps to the same channel num\n",
    "        self.in_proj = nn.ModuleList([]) # importante cuando guardemos listas de bloques\n",
    "        for i in range(len(in_dim_list)):\n",
    "            if self.spatial_dims == 2:\n",
    "                self.in_proj.append(nn.Conv2d(in_dim_list[i], dim, kernel_size=1, bias=False))\n",
    "            else:\n",
    "                self.in_proj.append(nn.Conv3d(in_dim_list[i], dim, kernel_size=1, bias=False))\n",
    "\n",
    "\n",
    "        self.transformer =  nn.Sequential(\n",
    "            *[torch.nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=dim, dropout=dropout, \n",
    "                activation='gelu', batch_first=True, norm_first=True) for k in range(4)])\n",
    "\n",
    "        # project all maps back to their origin channel num\n",
    "        self.out_proj = nn.ModuleList([])\n",
    "        for i in range(len(in_dim_list)):\n",
    "            if self.spatial_dims == 2:\n",
    "                self.out_proj.append(nn.Conv2d(dim, in_dim_list[i], kernel_size=1, bias=False))\n",
    "            else:\n",
    "                self.out_proj.append(nn.Conv3d(dim, in_dim_list[i], kernel_size=1, bias=False))\n",
    "\n",
    "    def forward(self, map_list):\n",
    "        if self.spatial_dims == 2:\n",
    "            b, c, h, w = map_list[0].shape\n",
    "            d = None\n",
    "        else:\n",
    "            b, c, d, h, w = map_list[0].shape\n",
    "\n",
    "        #proj_maps = [self.in_proj[i](map_list[i]).view(B, self.dim, -1).permute(0, 2, 1) for i in range(len(map_list))]\n",
    "        #B, L, C where L=DHW\n",
    "        proj_maps = [in_proj(map) for map, in_proj in  zip(map_list, self.in_proj)]\n",
    "        if self.spatial_dims == 2:\n",
    "            proj_maps = [rearrange(map, 'b c h w -> b (h w) c', h=h, w=w) for map in proj_maps]\n",
    "        else:\n",
    "            proj_maps = [rearrange(map, 'b c d h w -> b (d h w) c', d=d, h=h, w=w) for map in proj_maps]\n",
    "\n",
    "        proj_maps = torch.cat(proj_maps, dim=1) #all tokens as a 1D sequence\n",
    "        attned_maps = self.transformer(proj_maps)\n",
    "\n",
    "        attend_maps = attned_maps.chunk(len(map_list), dim=1) # split the tensor into maps of different resolutions\n",
    "\n",
    "        if self.spatial_dims == 2:\n",
    "            maps_out = [rearrange(attend_map, 'b (h w) c -> b c h w', h=h, w=w) for attend_map in attend_maps]\n",
    "        else:\n",
    "            maps_out = [rearrange(attend_map, 'b (d h w) c -> b c d h w', d=d, h=h, w=w) for attend_map in attend_maps]\n",
    "        \n",
    "        \n",
    "        \n",
    "        maps_out = [out_proj(attend_map) for out_proj, attend_map in zip(self.out_proj, maps_out)]\n",
    "\n",
    "        return maps_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "m = torch.randn(5, 20, 4, 4)\n",
    "M = [m, m, m]\n",
    "\n",
    "sF = SemanticMapFusion(2, [20, 20, 20], 20, 4, depth=4)\n",
    "\n",
    "Mo = sF(M)\n",
    "\n",
    "\n",
    "print(Mo[0].shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling and upsampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MedFormerEncoder(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, conv_num, trans_num, down_scale=[2,2,2], kernel_size=[3,3,3],\n",
    "                conv_block=BasicBlock, heads=4, dim_head=64, expansion=1, attn_drop=0., \n",
    "                proj_drop=0., map_size=[8,8,8], proj_type='depthwise', norm=nn.BatchNorm3d, \n",
    "                act=nn.GELU, map_generate=False, map_dim=None):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        map_dim = out_ch if map_dim is None else map_dim\n",
    "        self.map_generate = map_generate\n",
    "        if map_generate:\n",
    "            self.map_gen = SemanticMapGeneration(out_ch, map_dim, map_size)\n",
    "\n",
    "        self.patch_merging = PatchMerging(in_ch, out_ch, norm=norm, proj_type=proj_type, down_scale=down_scale, kernel_size=kernel_size)\n",
    "\n",
    "        block_list = []\n",
    "        for i in range(conv_num):\n",
    "            block_list.append(conv_block(out_ch, out_ch, norm=norm, act=act, kernel_size=kernel_size))\n",
    "        self.conv_blocks = nn.Sequential(*block_list)\n",
    "\n",
    "        self.trans_blocks = BasicLayer(out_ch, map_dim, out_ch, num_blocks=trans_num, heads=heads, \\\n",
    "                dim_head=dim_head, norm=norm, act=act, expansion=expansion, attn_drop=attn_drop, \\\n",
    "                proj_drop=proj_drop, map_size=map_size, proj_type=proj_type, kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_merging(x)\n",
    "\n",
    "        out = self.conv_blocks(x)\n",
    "\n",
    "        if self.map_generate:\n",
    "            semantic_map = self.map_gen(out)\n",
    "        else:\n",
    "            semantic_map = None\n",
    "\n",
    "        out, semantic_map = self.trans_blocks(out, semantic_map)\n",
    "            \n",
    "\n",
    "        return out, semantic_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (1): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (2): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (3): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MedFormerDecoder(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, conv_num, trans_num, up_scale=[2,2,2], kernel_size=[3,3,3], \n",
    "                conv_block=BasicBlock, heads=4, dim_head=64, expansion=4, attn_drop=0., proj_drop=0.,\n",
    "                map_size=[4,8,8], proj_type='depthwise', norm=nn.BatchNorm3d, act=nn.GELU, \n",
    "                map_dim=None, map_shortcut=False):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.map_shortcut = map_shortcut\n",
    "        map_dim = out_ch if map_dim is None else map_dim\n",
    "        if map_shortcut:\n",
    "            self.map_reduction = nn.Conv3d(in_ch+out_ch, map_dim, kernel_size=1, bias=False)\n",
    "        else:\n",
    "            self.map_reduction = nn.Conv3d(in_ch, map_dim, kernel_size=1, bias=False)\n",
    "\n",
    "        self.trans_blocks = BasicLayer(in_ch+out_ch, map_dim, out_ch, num_blocks=trans_num, \\\n",
    "\n",
    "                    heads=heads, dim_head=dim_head, norm=norm, act=act, expansion=expansion, \\\n",
    "                    attn_drop=attn_drop, proj_drop=proj_drop, map_size=map_size,\\\n",
    "                    proj_type=proj_type, kernel_size=kernel_size)\n",
    "\n",
    "        if trans_num == 0:\n",
    "            dim1 = in_ch+out_ch\n",
    "        else:\n",
    "            dim1 = out_ch\n",
    "\n",
    "        conv_list = []\n",
    "        for i in range(conv_num):\n",
    "            conv_list.append(conv_block(dim1, out_ch, kernel_size=kernel_size, norm=norm, act=act))\n",
    "            dim1 = out_ch\n",
    "        self.conv_blocks = nn.Sequential(*conv_list)\n",
    "\n",
    "    def forward(self, x1, x2, map1, map2=None):\n",
    "        # x1: low-res feature, x2: high-res feature shortcut from encoder\n",
    "        # map1: semantic map from previous low-res layer\n",
    "        # map2: semantic map from encoder shortcut, might be none if we don't have the map from encoder\n",
    "        \n",
    "        x1 = F.interpolate(x1, size=x2.shape[-3:], mode='trilinear', align_corners=True)\n",
    "        feat = torch.cat([x1, x2], dim=1)\n",
    "        \n",
    "        if self.map_shortcut and map2 is not None:\n",
    "            semantic_map = torch.cat([map1, map2], dim=1)\n",
    "        else:\n",
    "            semantic_map = map1\n",
    "        \n",
    "        if semantic_map is not None:\n",
    "            semantic_map = self.map_reduction(semantic_map)\n",
    "\n",
    "        out, semantic_map = self.trans_blocks(feat, semantic_map)\n",
    "        out = self.conv_blocks(out)\n",
    "\n",
    "        return out, semantic_map\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CursoTransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:35:26) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "332768db682295b8dac3c4a901a1fe1343c13865b4033bb10f86fd07133ed44e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
