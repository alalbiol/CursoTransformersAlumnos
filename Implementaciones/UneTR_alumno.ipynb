{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion UNETR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](figs/unetr.png)\n",
    "\n",
    "\n",
    "Todas las convoluciones tienen bias = False (logico ya que hay batchnorm)\n",
    "\n",
    "Diferencias entre dibujo e implementacion:\n",
    "\n",
    "* Emplea LeakyReLU en lugar de ReLU\n",
    "* El primer bloque azul de cada encoder simplemente es un TransposeConv para reducir de 768 al numero de canales de la capa\n",
    "* El resto de las cajas azules tienen el TransposeConv y dos bloques amarillos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secuencia bloques amarillos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, Union, Optional\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class BloquesAmarillos(nn.Sequential):\n",
    "    \"\"\"\n",
    "    # Implementacion de la secuencia de dos bloques amarillos. Cada bloque amarillo es Conv3x3x3 BN LeakyRelu\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions.\n",
    "        in_channels: number of input channels.\n",
    "        out_channels: number of output channels.\n",
    "        kernel_size: convolution kernel size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,    \n",
    "    ):\n",
    "        padding = kernel_size // 2\n",
    "        if spatial_dims == 2:\n",
    "            # completar por alumno los bloques\n",
    "            module_list = []\n",
    "        elif spatial_dims == 3:\n",
    "            module_list = []\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unsupported spatial_dims: {}\".format(spatial_dims))\n",
    "        super().__init__(*module_list)\n",
    "\n",
    "    # no hace falta definir forward porque nn.Sequential ya lo tiene definido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BloquesAmarillos(\n",
      "  (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "  (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "  (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bloque_amarillo = BloquesAmarillos(3, 16, 32)\n",
    "\n",
    "print(bloque_amarillo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Corresponde con los bloques azules de la figura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa la secuencia bloques azules. El primer bloque azul es unicamente un convtranspose2d \n",
    "    Luego los siguientes cada bloque azul es un convtranspose2d y dos bloques amarillos.\n",
    "    ES DIFERENTE A LO QUE PONE EN LA FIGURA!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        num_bloques_azules: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spatial_dims: number of spatial dimensions.\n",
    "            in_channels: number of input channels.\n",
    "            out_channels: number of output channels.\n",
    "            num_layer: number of upsampling blocks.\n",
    "            kernel_size: convolution kernel size.\n",
    "            stride: convolution stride.\n",
    "            upsample_kernel_size: convolution kernel size for transposed convolution layers.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        if spatial_dims == 2:\n",
    "            self.primer_bloque = \n",
    "\n",
    "            #este sera un nn.ModuleList (ver forward)\n",
    "            self.blocks = \n",
    "                    \n",
    "            \n",
    "        else:\n",
    "            # completar por alumno para 3d\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.primer_bloque(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobacion\n",
    "El número de parametros tiene que salir 2819072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (primer_bloque): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      (1): BloquesAmarillos(\n",
      "        (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "        (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      (1): BloquesAmarillos(\n",
      "        (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "        (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2819072\n"
     ]
    }
   ],
   "source": [
    "encoder2 = Encoder(3,768, 128, 3)\n",
    "print(encoder2)\n",
    "\n",
    "num_params = sum(p.numel() for p in encoder2.parameters() if p.requires_grad)\n",
    "print(\"num params = \", num_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "![alt](figs/decoder.png)\n",
    "\n",
    "* Esta formado por un bloque verde que interpola y reduce canales\n",
    "* En la interpolacion se reduce el numero de canales para que coincida con el numero de canales de la capa anterior\n",
    "* Dos bloques amarillos que hacen lo mismo que los de antes\n",
    "* El número de canales en la entrada de los bloques amarillos es el doble que en el encoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa el decoder. Primero hace un upsampling del nivel inferior, luego lo combina con el nivel superior y finalmente\n",
    "    dos bloques amarillos\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        in_channels_down: int,\n",
    "        out_channels: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spatial_dims: number of spatial dimensions.\n",
    "            in_channels: number of input channels.\n",
    "            in_cnannels_down: number of input channels from the lower level.\n",
    "            out_channels: number of output channels.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        if spatial_dims == 2:\n",
    "            self.upsample = nn.ConvTranspose2d(in_channels_down, in_channels, 2, 2, bias=False)\n",
    "            self.decoder = BloquesAmarillos(spatial_dims, 2*in_channels, out_channels)\n",
    "        else:\n",
    "            self.upsample = nn.ConvTranspose3d(in_channels_down, in_channels, 2, 2, bias=False)\n",
    "            self.decoder = BloquesAmarillos(spatial_dims, 2*in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_down, x):\n",
    "        x2 = self.upsample(x_down)\n",
    "        x3 = out = torch.cat((x2,x), dim=1)\n",
    "        out = self.decoder(x3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (upsample): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "  (decoder): BloquesAmarillos(\n",
      "    (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n",
      "6358016\n"
     ]
    }
   ],
   "source": [
    "decoder5 = Decoder(3, 256, 512, 256)\n",
    "print(decoder5)\n",
    "\n",
    "num_params = sum(p.numel() for p in decoder5.parameters() if p.requires_grad)\n",
    "print(num_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, Union\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from monai.networks.nets.vit import ViT\n",
    "\n",
    "\n",
    "# para que funcion tanto si spatial_dims es 2 como 3\n",
    "from monai.utils.misc import ensure_tuple_rep\n",
    "\n",
    "\n",
    "class myUNETR(nn.Module):\n",
    "    \"\"\"\n",
    "    UNETR based on: \"Hatamizadeh et al.,\n",
    "    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        img_size: Union[Sequence[int], int],\n",
    "        feature_size: int = 64,\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"conv\",\n",
    "        norm_name: Union[Tuple, str] = \"instance\",\n",
    "        conv_block: bool = True,\n",
    "        res_block: bool = True,\n",
    "        dropout_rate: float = 0.0,\n",
    "        spatial_dims: int = 3,\n",
    "        qkv_bias: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels.\n",
    "            out_channels: dimension of output channels.\n",
    "            img_size: dimension of input image.\n",
    "            feature_size: dimension of network feature size.\n",
    "            hidden_size: dimension of hidden layer.\n",
    "            mlp_dim: dimension of feedforward layer.\n",
    "            num_heads: number of attention heads.\n",
    "            pos_embed: position embedding layer type.\n",
    "            norm_name: feature normalization type and arguments.\n",
    "            conv_block: bool argument to determine if convolutional block is used.\n",
    "            res_block: bool argument to determine if residual block is used.\n",
    "            dropout_rate: faction of the input units to drop.\n",
    "            spatial_dims: number of spatial dims.\n",
    "            qkv_bias: apply the bias term for the qkv linear layer in self attention block\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # for single channel input 4-channel output with image size of (96,96,96), feature size of 32 and batch norm\n",
    "            >>> net = UNETR(in_channels=1, out_channels=4, img_size=(96,96,96), feature_size=32, norm_name='batch')\n",
    "\n",
    "             # for single channel input 4-channel output with image size of (96,96), feature size of 32 and batch norm\n",
    "            >>> net = UNETR(in_channels=1, out_channels=4, img_size=96, feature_size=32, norm_name='batch', spatial_dims=2)\n",
    "\n",
    "            # for 4-channel input 3-channel output with image size of (128,128,128), conv position embedding and instance norm\n",
    "            >>> net = UNETR(in_channels=4, out_channels=3, img_size=(128,128,128), pos_embed='conv', norm_name='instance')\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise ValueError(\"dropout_rate should be between 0 and 1.\")\n",
    "\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise ValueError(\"hidden_size should be divisible by num_heads.\")\n",
    "\n",
    "        self.num_layers = 12\n",
    "        img_size = ensure_tuple_rep(img_size, spatial_dims)\n",
    "        self.patch_size = ensure_tuple_rep(16, spatial_dims)\n",
    "        self.feat_size = tuple(img_d // p_d for img_d, p_d in zip(img_size, self.patch_size))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.classification = False\n",
    "        self.vit = ViT(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=self.patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=num_heads,\n",
    "            pos_embed=pos_embed,\n",
    "            classification=self.classification,\n",
    "            dropout_rate=dropout_rate,\n",
    "            spatial_dims=spatial_dims,\n",
    "            qkv_bias=qkv_bias,\n",
    "        )\n",
    "        self.encoder1 = BloquesAmarillos(  # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "        )\n",
    "        self.encoder2 = Encoder( # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 2,\n",
    "            num_bloques_azules=3,\n",
    "        )\n",
    "        self.encoder3 = Encoder( # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 4,\n",
    "            num_bloques_azules=2,\n",
    "        )\n",
    "        self.encoder4 = Encoder( # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            num_bloques_azules=1,\n",
    "        )\n",
    "        \n",
    "        self.decoder5 = Decoder( # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            in_channels_down=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "        )\n",
    "        self.decoder4 = Decoder( # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            in_channels_down=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "        )\n",
    "        self.decoder3 = Decoder( # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            in_channels_down=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "        )\n",
    "\n",
    "        self.decoder2 = Decoder( # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size ,\n",
    "            in_channels_down=feature_size * 2,\n",
    "            out_channels=feature_size ,\n",
    "        )\n",
    "        if spatial_dims == 2:\n",
    "            self.out = nn.Conv2d( # type: ignore\n",
    "                in_channels=feature_size ,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                bias=True,\n",
    "            )\n",
    "        else:\n",
    "            self.out = nn.Conv3d( # type: ignore\n",
    "                in_channels=feature_size ,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                bias=True,\n",
    "            )\n",
    "\n",
    "        \n",
    "        self.proj_axes = (0, spatial_dims + 1) + tuple(d + 1 for d in range(spatial_dims))\n",
    "        self.proj_view_shape = list(self.feat_size) + [self.hidden_size]\n",
    "\n",
    "    def proj_feat(self, x):\n",
    "        new_view = [x.size(0)] + self.proj_view_shape\n",
    "        x = x.view(new_view)\n",
    "        x = x.permute(self.proj_axes).contiguous()\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x, hidden_states_out = self.vit(x_in)\n",
    "        enc1 = self.encoder1(x_in)\n",
    "        x2 = hidden_states_out[3]\n",
    "        enc2 = self.encoder2(self.proj_feat(x2))\n",
    "        x3 = hidden_states_out[6]\n",
    "        enc3 = self.encoder3(self.proj_feat(x3))\n",
    "        x4 = hidden_states_out[9]\n",
    "        enc4 = self.encoder4(self.proj_feat(x4))\n",
    "        dec4 = self.proj_feat(x)\n",
    "        dec3 = self.decoder5(dec4, enc4)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        out = self.decoder2(dec1, enc1)\n",
    "        return self.out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myUNETR(in_channels=4, out_channels=3, img_size=(128,128,128), feature_size=64, norm_name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 142451907\n",
      "Number of trainable parameters: 117760\n",
      "Number of trainable parameters: 2819072\n",
      "Number of trainable parameters: 5637120\n",
      "Number of trainable parameters: 3145728\n"
     ]
    }
   ],
   "source": [
    "blocks = [model, model.encoder1, model.encoder2, model.encoder3, model.encoder4]\n",
    "\n",
    "for block in blocks:\n",
    "    num_params = sum(p.numel() for p in block.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variaciones\n",
    "\n",
    "* Cambiar el tipo de normalización\n",
    "* Cambiar el tipo de activación\n",
    "* Bloques amarillos con conexión residual\n",
    "* Bloques azules con conexión residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('CursoTransformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:35:26) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "332768db682295b8dac3c4a901a1fe1343c13865b4033bb10f86fd07133ed44e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
